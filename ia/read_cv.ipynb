{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read PDF\n",
    "\n",
    "# from dotenv import load_dotenv\n",
    "# Charger les variables d'environnement\n",
    "# load_dotenv()\n",
    "# Faire lettre motivation automatique\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def read_pdf(file_path: str) -> str:\n",
    "    \"\"\"Lit et extrait le texte d'un CV au format PDF d'une seule page.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Chemin d'accès vers le fichier PDF à lire\n",
    "\n",
    "    Returns:\n",
    "        str: Texte brut extrait du PDF\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: Si le fichier PDF n'existe pas\n",
    "        IndexError: Si le PDF est vide\n",
    "        Exception: Pour toute autre erreur lors de la lecture du PDF\n",
    "    \"\"\"\n",
    "    try:\n",
    "        reader = PdfReader(file_path)\n",
    "        if len(reader.pages) == 0:\n",
    "            raise IndexError(\"Le PDF est vide\")\n",
    "        \n",
    "        page = reader.pages[0]  # Lecture de la première page uniquement\n",
    "        text_brut = page.extract_text()\n",
    "        \n",
    "        if not text_brut:\n",
    "            raise Exception(\"Aucun texte n'a pu être extrait du PDF\")\n",
    "            \n",
    "        return text_brut\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Le fichier {file_path} n'existe pas\")\n",
    "    except IndexError as e:\n",
    "        raise IndexError(str(e))\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Erreur lors de la lecture du PDF: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quentin Lim  \n",
      "Last year Student in \n",
      "Data Science\n",
      "quentinlim 384@ gm ail.com\n",
      "+33782885515\n",
      "linkedin.com /in/quentin-lim-\n",
      "978746250\n",
      "github.com /QL2111\n",
      "Profile\n",
      "Currently seeking an internship in \n",
      "Data Science starting in March \n",
      "2025 for a duration of 4 to 6 \n",
      "months.\n",
      "Skills\n",
      "M L, NLP, TensorFlow; Scikit-learn, \n",
      "M LFlow, Transform ers, LLM , Tim e \n",
      "Series.\n",
      "SQL, NoSQL, M ongoDB\n",
      "Qlik, Tableau, PowerBI\n",
      "OpenCV, YOLO\n",
      "Azure, AW S, Fabric\n",
      "Git/Agile m ethodology\n",
      "Python/R/Java/PySpark/JavaScript/\n",
      "PHP\n",
      "Gen AI\n",
      "Interests\n",
      "Board Games: Casual boarding\n",
      "gam es and Chess club\n",
      "Gaming: Played in competitives\n",
      "tournam ents, peaked GrandMaster\n",
      "in League of Legends\n",
      "Sailing: M em ber of the sailing club\n",
      "at La Rochelle Université\n",
      "Basket ball: Sports association\n",
      "during M iddle School, teamwork\n",
      "and fast decision making.\n",
      "Volunteering\n",
      "ESN Cosmolyon\n",
      "2023 – present\n",
      "Vice-President, holdings Culturals \n",
      "Event for internationals and local \n",
      "student(Board gam es/Day \n",
      "trips/Spanish dinner)\n",
      "Alter-ego Lyon 2\n",
      "2023 – present\n",
      "Buddy System  with Lyon 2 \n",
      "UniversityEducation\n",
      "Master in Data Science, Université Lyon 2 Lumière\n",
      "09/2023 – present\n",
      "•Expertise in advanced data analysis: ANOVA, time series, biostatistics, NLP/LLM, \n",
      "Computer Vision, and Deep Learning.\n",
      "•Proficiency in Big Data and MLOps: web scraping, data warehouses, model \n",
      "deployment, and pipelines.\n",
      "•Specialized in predictive analytics, focusing on the use of advanced statistical \n",
      "models and machine learning algorithms for forecasting and decision making.\n",
      "International Exchange Program, National Central University\n",
      "Taoyuan, Taïwan\n",
      "•GPA: 3.8 | Experience in data mining for biological data: autism classification, \n",
      "sentiment analysis and Graph Neural Networks presentation on Reddit data.\n",
      "•Research & Publications: Conducted a literature review on feature selection \n",
      "techniques and co-authored \"A Review of Feature Selection Techniques in Education\". \n",
      "Explored Transformer models for advanced data analysis.\n",
      "Erasmus + Mobility, South East Technological University\n",
      "Waterford, Ireland\n",
      "•First Class Honours | EDA & predictive modeling on Titanic and Tips datasets with \n",
      "up to 90% accuracy, NoSQL database creation for TFI bikes in Waterford, and data \n",
      "visualization in R with NYC flights data.\n",
      "•DevOps on AWS: EC2 instances, S3 buckets, configuration, and achieved \n",
      "automation with Python scripts.\n",
      "Professional Experience\n",
      "University Informatics Department, Lyon 2\n",
      "present\n",
      "Student Job at the IT department of the university, help desk, loan of computer and \n",
      "professional audiovisual equiment, teamwork.\n",
      "Internship Data Analyst, SYSBLOCK\n",
      "Paris, France\n",
      "•Dashboarding & Analytics: KPI derivation from Google Analytics data and \n",
      "interactive dashboards with Power BI.\n",
      "•Web Development & Blockchain: Experience with Next.js for web development and \n",
      "introduction to blockchain technologies.\n",
      "Projects\n",
      "Creation of a Logistic Regression R package\n",
      "•Logistic regression package from scratch :Implemented various optimizers \n",
      "(Adam, mini-batch, SGD) and applied regularization techniques (L1, L2, ElasticNet). \n",
      "•Successfully tested on classification tasks : achieved up to 90% accuracy on \n",
      "StudentPerformance, Creditcard,  datasets and achieved similar performance to \n",
      "scikit-learn..\n",
      "Projet NLP Analysis of TripAdvisor review\n",
      "•Web Scraping & Sentiment Analysis: Collected TripAdvisor data with \n",
      "BeautifulSoup and performed sentiment analysis on user reviews. Aggregated \n",
      "comments using clustering techniques.\n",
      "•End-to-End Application: Integrated a SQLite database and connected it to a \n",
      "Streamlit app for enhanced visualization. Deployed as a Docker image.\n",
      "•LLM & RAG for Summarization: Leveraged a Large Language Model (LLM) with \n",
      "Retrieval-Augmented Generation (RAG) to summarize user reviews effectively.\n",
      "Project AWS Cloud\n",
      "•AWS Deployment & Scaling: Successfully deployed a template website on EC2 \n",
      "instances with a connected relational database on AWS.\n",
      "•High Availability & Monitoring: Implemented an Auto Scaling Group with a Load \n",
      "Balancer and set up monitoring using custom CloudWatch metrics.\n",
      "Languages/References\n",
      "Languages: French - Native/Bilingual   English C1 - Fluent  Mandarin -\n",
      "Conversational\n",
      "References: Bernard Butler, Pr, SETU  bernard.butler@setu.ie   Liam Doyle, Pr,\n",
      "SETU  liam.doyle@setu.ie\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "<class 'str'>\n",
      "4231\n"
     ]
    }
   ],
   "source": [
    "reader = PdfReader(\"CV_V4_EN.pdf\")\n",
    "page = reader.pages[0] # 1 seule page\n",
    "text_brut = page.extract_text()\n",
    "print(text_brut)\n",
    "print(type(text_brut))\n",
    "print(len(text_brut))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(read_pdf(\"CV_V4_EN.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Mistral Mini -> Reformulation Texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Formulation du prompt (CV texte brut + )\n",
    "# Ajouter les suivis de prix + impact ecologique ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import litellm\n",
    "import json\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nA partir d\\'un texte brut venant d\\'un CV, extrait les informations suivantes au format JSON :\\n{\\n    \"Diplome\": [\\n      {\\n          \"niveau_etudes\": \"int\",\\n          \"domaine_etudes\": [\"str\"]\\n      }\\n    ],\\n\\n    \"Competences\": [\"str\"],\\n    \"Experiences\": [\\n        {\\n            \"domaine_activite\": [\"str\"],\\n            \"poste_occupe\": \"str\",\\n            \"duree\": \"int\"\\n        }\\n    ],\\n    \"Profil\": {\\n        \"titre\": \"str\",\\n        \"disponibilite\": \"str\"\\n    }\\n}\\n\\nTexte du CV :\\n\"Quentin Lim  \\nLast year Student in \\nData Science\\nquentinlim 384@ gm ail.com\\n+33782885515\\nlinkedin.com /in/quentin-lim-\\n978746250\\ngithub.com /QL2111\\nProfile\\nCurrently seeking an internship in \\nData Science starting in March \\n2025 for a duration of 4 to 6 \\nmonths.\\nSkills\\nM L, NLP, TensorFlow; Scikit-learn, \\nM LFlow, Transform ers, LLM , Tim e \\nSeries.\\nSQL, NoSQL, M ongoDB\\nQlik, Tableau, PowerBI\\nOpenCV, YOLO\\nAzure, AW S, Fabric\\nGit/Agile m ethodology\\nPython/R/Java/PySpark/JavaScript/\\nPHP\\nGen AI\\nInterests\\nBoard Games:\\xa0Casual boarding\\ngam es and Chess club\\nGaming:\\xa0Played in competitives\\ntournam ents, peaked GrandMaster\\nin League of Legends\\nSailing:\\xa0M em ber of the sailing club\\nat La Rochelle Université\\nBasket ball:\\xa0Sports association\\nduring M iddle School, teamwork\\nand fast decision making.\\nVolunteering\\nESN Cosmolyon\\n2023 – present\\nVice-President, holdings Culturals \\nEvent for internationals and local \\nstudent(Board gam es/Day \\ntrips/Spanish dinner)\\nAlter-ego Lyon 2\\n2023 – present\\nBuddy System  with Lyon 2 \\nUniversityEducation\\nMaster in Data Science, Université Lyon 2 Lumière\\n09/2023 – present\\n•Expertise in advanced data analysis: ANOVA, time series, biostatistics, NLP/LLM, \\nComputer Vision, and Deep Learning.\\n•Proficiency in Big Data and MLOps: web scraping, data warehouses, model \\ndeployment, and pipelines.\\n•Specialized in predictive analytics, focusing on the use of advanced statistical \\nmodels and machine learning algorithms for forecasting and decision making.\\nInternational Exchange Program, National Central University\\nTaoyuan, Taïwan\\n•GPA: 3.8 | Experience in data mining for biological data: autism classification, \\nsentiment analysis and Graph Neural Networks presentation on Reddit data.\\n•Research & Publications: Conducted a literature review on feature selection \\ntechniques and co-authored \"A Review of Feature Selection Techniques in Education\". \\nExplored Transformer models for advanced data analysis.\\nErasmus + Mobility, South East Technological University\\nWaterford, Ireland\\n•First Class Honours | EDA & predictive modeling on Titanic and Tips datasets with \\nup to 90% accuracy, NoSQL database creation for TFI bikes in Waterford, and data \\nvisualization in R with NYC flights data.\\n•DevOps on AWS: EC2 instances, S3 buckets, configuration, and achieved \\nautomation with Python scripts.\\nProfessional Experience\\nUniversity Informatics Department, Lyon 2\\npresent\\nStudent Job at the IT department of the university, help desk, loan of computer and \\nprofessional audiovisual equiment, teamwork.\\nInternship Data Analyst, SYSBLOCK\\nParis, France\\n•Dashboarding & Analytics: KPI derivation from Google Analytics data and \\ninteractive dashboards with Power BI.\\n•Web Development & Blockchain: Experience with Next.js for web development and \\nintroduction to blockchain technologies.\\nProjects\\nCreation of a Logistic Regression R package\\n•Logistic regression package from scratch :Implemented various optimizers \\n(Adam, mini-batch, SGD) and applied regularization techniques (L1, L2, ElasticNet). \\n•Successfully tested on classification tasks : achieved up to 90% accuracy on \\nStudentPerformance, Creditcard,  datasets and achieved similar performance to \\nscikit-learn..\\nProjet NLP Analysis of TripAdvisor review\\n•Web Scraping & Sentiment Analysis: Collected TripAdvisor data with \\nBeautifulSoup and performed sentiment analysis on user reviews. Aggregated \\ncomments using clustering techniques.\\n•End-to-End Application: Integrated a SQLite database and connected it to a \\nStreamlit app for enhanced visualization. Deployed as a Docker image.\\n•LLM & RAG for Summarization: Leveraged a Large Language Model (LLM) with \\nRetrieval-Augmented Generation (RAG) to summarize user reviews effectively.\\nProject AWS Cloud\\n•AWS Deployment & Scaling: Successfully deployed a template website on EC2 \\ninstances with a connected relational database on AWS.\\n•High Availability & Monitoring: Implemented an Auto Scaling Group with a Load \\nBalancer and set up monitoring using custom CloudWatch metrics.\\nLanguages/References\\nLanguages:\\xa0French - Native/Bilingual \\xa0\\xa0English C1 - Fluent\\xa0\\xa0Mandarin -\\nConversational\\nReferences:\\xa0Bernard Butler, Pr, SETU\\xa0\\xa0bernard.butler@setu.ie \\xa0\\xa0Liam Doyle, Pr,\\nSETU\\xa0\\xa0liam.doyle@setu.ie\\n|\\n|\\n|\\n|\\n|\"\\n\\nNe retourne que le JSON\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reformulation_prompt = f\"\"\"\n",
    "A partir d'un texte brut venant d'un CV, extrait les informations suivantes au format JSON :\n",
    "{{\n",
    "    \"Diplome\": [\n",
    "      {{\n",
    "          \"niveau_etudes\": \"int\",\n",
    "          \"domaine_etudes\": [\"str\"]\n",
    "      }}\n",
    "    ],\n",
    "\n",
    "    \"Competences\": [\"str\"],\n",
    "    \"Experiences\": [\n",
    "        {{\n",
    "            \"domaine_activite\": [\"str\"],\n",
    "            \"poste_occupe\": \"str\",\n",
    "            \"duree\": \"int\"\n",
    "        }}\n",
    "    ],\n",
    "    \"Profil\": {{\n",
    "        \"titre\": \"str\",\n",
    "        \"disponibilite\": \"str\"\n",
    "    }}\n",
    "}}\n",
    "\n",
    "Texte du CV :\n",
    "\"{text_brut}\"\n",
    "\n",
    "Ne retourne que le JSON\n",
    "\"\"\"\n",
    "reformulation_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification de la clé API\n",
    "#  api_key=os.getenv(\"MISTRAL_API_KEY\")\n",
    "# api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# litellm._turn_on_debug()\n",
    "# Température, plus c'est proche de 1, plus c'est créatif\n",
    "# On cherche à savoir combien de token on devrait mettre\n",
    "\n",
    "CV_reformuler = litellm.completion(\n",
    "            model=\"mistral/ministral-3b-latest\",  # Choix d'un modèle petit\n",
    "            messages=[{\"role\": \"user\", \"content\": reformulation_prompt}],\n",
    "            max_tokens=1500,\n",
    "            temperature=0.3,\n",
    "            api_key=os.getenv(\"MISTRAL_API_KEY\"),\n",
    "        )\n",
    "\n",
    "resultat = CV_reformuler[\"choices\"][0][\"message\"][\n",
    "            \"content\"\n",
    "        ].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"Diplome\": [\n",
      "        {\n",
      "            \"niveau_etudes\": \"Master\",\n",
      "            \"domaine_etudes\": [\"Data Science\"]\n",
      "        }\n",
      "    ],\n",
      "    \"Competences\": [\n",
      "        \"M L, NLP, TensorFlow; Scikit-learn, M LFlow, Transform ers, LLM , Tim e Series.\",\n",
      "        \"SQL, NoSQL, M ongoDB\",\n",
      "        \"Qlik, Tableau, PowerBI\",\n",
      "        \"OpenCV, YOLO\",\n",
      "        \"Azure, AW S, Fabric\",\n",
      "        \"Git/Agile m ethodology\",\n",
      "        \"Python/R/Java/PySpark/JavaScript/\",\n",
      "        \"PHP\",\n",
      "        \"Gen AI\"\n",
      "    ],\n",
      "    \"Experiences\": [\n",
      "        {\n",
      "            \"domaine_activite\": [\"Data Science\"],\n",
      "            \"poste_occupe\": \"Student Job at the IT department of the university\",\n",
      "            \"duree\": 1\n",
      "        },\n",
      "        {\n",
      "            \"domaine_activite\": [\"Data Science\"],\n",
      "            \"poste_occupe\": \"Internship Data Analyst\",\n",
      "            \"duree\": 1\n",
      "        }\n",
      "    ],\n",
      "    \"Profil\": {\n",
      "        \"titre\": \"Currently seeking an internship in Data Science starting in March 2025 for a duration of 4 to 6 months.\",\n",
      "        \"disponibilite\": \"March 2025\"\n",
      "    }\n",
      "}\n",
      "```\n",
      "1032\n"
     ]
    }
   ],
   "source": [
    "# Le CV fait 4231 en len\n",
    "# len de 1863 pour 500 max tokens\n",
    "# len de 4429 pour 1500 max tokens -> C'est OK, on essaye avec 1000 max tokens\n",
    "print(resultat)\n",
    "print(len(resultat))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cv(text_brut: str, temperature: float = 0.01, max_tokens: int = 1500) -> str:\n",
    "    \"\"\"\n",
    "    Analyse un CV en format texte et retourne les informations structurées en JSON.\n",
    "\n",
    "    Args:\n",
    "        text_brut (str): Texte brut du CV à analyser\n",
    "        temperature (float, optional): Paramètre de créativité du modèle. Defaults to 0.2.\n",
    "        max_tokens (int, optional): Nombre maximum de tokens pour la réponse. Defaults to 1500.\n",
    "\n",
    "    Returns:\n",
    "        str: Informations en string(réponse du LLM) structurées du CV au format JSON\n",
    "\n",
    "    Raises:\n",
    "        Exception: En cas d'erreur d'API ou de parsing JSON\n",
    "    \"\"\"\n",
    "    try:\n",
    "        reformulation_prompt = f\"\"\"\n",
    "        À partir du texte brut d'un CV, extrais les informations suivantes au format JSON. \n",
    "        Si une information n'est pas explicitement mentionnée dans le texte, retourne `null` ou une liste vide.\n",
    "        Ne devine pas les informations manquantes et ne retourne que ce qui est clairement présent dans le texte.\n",
    "\n",
    "        Format JSON attendu :\n",
    "        {{\n",
    "            \"Formation\": [\n",
    "                {{\n",
    "                    \"niveau_etudes\": \"str\",  // Ex: \"Licence\", \"Master\", \"Doctorat\"\n",
    "                    \"domaine_etudes\": [\"str\"]  // Ex: [\"Informatique\", \"Mathématiques\"]\n",
    "                }}\n",
    "            ],\n",
    "            \"Competences\": [\"str\"],  // Ex: [\"Python\", \"Gestion de projet\"]\n",
    "            \"Experiences\": [\n",
    "                {{\n",
    "                    \"domaine_activite\": [\"str\"],  // Ex: [\"Tech\", \"Finance\"]\n",
    "                    \"poste_occupe\": \"str\",  // Ex: \"Développeur Python\"\n",
    "                    \"duree\": \"str\"  // Ex: \"2 ans\"\n",
    "                }}\n",
    "            ],\n",
    "            \"Profil\": {{\n",
    "                \"titre\": \"str\",  // Ex: \"Développeur Full-Stack\"\n",
    "                \"disponibilite\": \"str\"  // Ex: \"dd-mm-yyyy\"\n",
    "            }}\n",
    "        }}\n",
    "\n",
    "        Texte du CV :\n",
    "        \"{text_brut}\"\n",
    "\n",
    "        Ne retourne que le JSON, sans commentaires supplémentaires.\n",
    "        \"\"\"\n",
    "\n",
    "        CV_reformuler = litellm.completion(\n",
    "            model=\"mistral/mistral-medium\",\n",
    "            messages=[{\"role\": \"user\", \"content\": reformulation_prompt}],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            api_key=os.getenv(\"MISTRAL_API_KEY\"),\n",
    "        )\n",
    "\n",
    "        # Extraire et parser le JSON de la réponse\n",
    "        resultat = CV_reformuler[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        return resultat\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Erreur lors de l'analyse du CV: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Formation\": [\n",
      "    {\n",
      "      \"niveau_etudes\": \"Master\",\n",
      "      \"domaine_etudes\": [\"Data Science\"]\n",
      "    }\n",
      "  ],\n",
      "  \"Competences\": [\n",
      "    \"ML\",\n",
      "    \"NLP\",\n",
      "    \"TensorFlow\",\n",
      "    \"Scikit-learn\",\n",
      "    \"MLFlow\",\n",
      "    \"Transformers\",\n",
      "    \"LLM\",\n",
      "    \"Time Series\",\n",
      "    \"SQL\",\n",
      "    \"NoSQL\",\n",
      "    \"MongoDB\",\n",
      "    \"Qlik\",\n",
      "    \"Tableau\",\n",
      "    \"PowerBI\",\n",
      "    \"OpenCV\",\n",
      "    \"YOLO\",\n",
      "    \"Azure\",\n",
      "    \"AWS\",\n",
      "    \"Fabric\",\n",
      "    \"Git/Agile methodology\",\n",
      "    \"Python\",\n",
      "    \"R\",\n",
      "    \"Java\",\n",
      "    \"PySpark\",\n",
      "    \"JavaScript\",\n",
      "    \"PHP\",\n",
      "    \"Gen AI\"\n",
      "  ],\n",
      "  \"Experiences\": [\n",
      "    {\n",
      "      \"domaine_activite\": [],\n",
      "      \"poste_occupe\": \"Vice-President\",\n",
      "      \"duree\": \"2023 – present\"\n",
      "    },\n",
      "    {\n",
      "      \"domaine_activite\": [],\n",
      "      \"poste_occupe\": \"Buddy System\",\n",
      "      \"duree\": \"2023 – present\"\n",
      "    },\n",
      "    {\n",
      "      \"domaine_activite\": [\"Data Science\"],\n",
      "      \"poste_occupe\": \"Student Job at the IT department\",\n",
      "      \"duree\": \"present\"\n",
      "    },\n",
      "    {\n",
      "      \"domaine_activite\": [\"Data Science\"],\n",
      "      \"poste_occupe\": \"Internship Data Analyst\",\n",
      "      \"duree\": \"Dashboarding & Analytics: KPI derivation from Google Analytics data and interactive dashboards with Power BI.\"\n",
      "    }\n",
      "  ],\n",
      "  \"Profil\": {\n",
      "    \"titre\": \"Currently seeking an internship in Data Science\",\n",
      "    \"disponibilite\": \"starting in March 2025 for a duration of 4 to 6 months.\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "CV_reformuler = analyze_cv(text_brut)\n",
    "print(CV_reformuler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matching CV et offre :\n",
    "\n",
    "- Obtention du string job offer par le scrapping\n",
    "- Transformation via un LLM de l'offre à un format similaire au CV_reformuler\n",
    "- Embedding + calcul de la similarité cosinus globale\n",
    "- Transformation en JSON du CV_reformuler et job_offer_reforumuler pour pouvoir effectuer une similarité par partie(clef communes car passé tout les deux par un LLM)\n",
    "- Calcul des similarité par parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Example job offer\n",
    "job_offer = \"\"\"\n",
    "  Titre : Data scientist (H/F) SALON TAF Toulouse 2025\n",
    "  Entreprise : AOSIS CONSULTING\n",
    "  Contrat : CDI\n",
    "  Localisation : 31 - TOULOUSE\n",
    "  Description :\n",
    "  Concrètement qu'est-ce qu'on fait ?\n",
    "  Nous proposons des Services, des Logiciels et de la Formation dans le domaine de la Business Intelligence, de la Data Science, de la Gouvernance et de la Visualisation de la donnée. L'Infrastructure et le Développement font également partie du panel de nos compétences.\n",
    "  Notre leitmotiv ? Citoyenneté, écologie et sport avec des experts de la donnée.\n",
    "  Notre objectif ? Porter encore plus haut et plus fort nos valeurs de sport, d'écologie et de citoyenneté et accompagner nos consultants dans leur montée en compétence.\n",
    "\n",
    "  Le poste\n",
    "  Dans le cadre d'une ouverture de poste, nous sommes à la recherche d'un.e Data Scientist pour l'un de nos clients.\n",
    "  Un.e data scientist senior (5 ans d'expérience) très autonome dans ses analyses et explorations, expérimenté dans l'industrialisation des modèles dans des environnements cloud.\n",
    "\n",
    "  Vous interviendrez sur toutes les étapes du projet, vous devrez :\n",
    "\n",
    "  Identifier leurs difficultés et proposer des solutions\n",
    "  Evaluer la qualité des données, les nettoyer, les agréger, et mener des études adhoc\n",
    "  Concevoir, implémenter et comparer différents algorithmes et méthodes statistiques\n",
    "  Concevoir et mener des protocoles de test en conditions réelles (magasins, entrepôts, .)\n",
    "  Développer des outils de visualisation des données\n",
    "  Mettre en production, maintenir et itérer sur les solutions\n",
    "\n",
    "  Environnement technique: Python, SQL, Terraform, Bitbucket, Jenkins, Airflow, Docker, Kubernetes, GCP (BigQuery, Spark DataProc)\n",
    "\n",
    "  Profil recherché\n",
    "  Vous êtes issu.e d'un Master en informatique ou mathématiques appliquées.\n",
    "  Vous avez à minima 5 années d'expériences en Data Science et maitrisez les algorithmes d'apprentissage statistique.\n",
    "  Vous maîtrisez Python et SQL\n",
    "  Vous avez une expérience dans le développement logiciel agile avec multiples contributeurs\n",
    "  Vous avez une expérience du cloud, idéalement GCP\n",
    "\n",
    "  Pourquoi nous rejoindre ?\n",
    "  Parce que nous sommes une société à taille humaine\n",
    "  Parce que nous nous engageons (réellement) dans des enjeux importants : l'écologie, la citoyenneté et le sport\n",
    "  Mais aussi parce que nous aimons réaliser tout un tas d'activités : des jeux, des afterworks, des restaurants, ... et plein d'autres choses encore :)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reformulation de l'offre d'emploi :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Transformation de l'offre d'emploi au même format que le CV\n",
    "def analyze_cv_offre_emploi(offre: str, temperature: float = 0.01, max_tokens: int = 1500) -> str:\n",
    "    \"\"\"\n",
    "    Analyse une offre d'emploi en format texte et retourne les informations structurées en JSON.\n",
    "\n",
    "    Args:\n",
    "        text_brut (str): Texte brut del'offre à analyser\n",
    "        temperature (float, optional): Paramètre de créativité du modèle. Defaults to 0.2.\n",
    "        max_tokens (int, optional): Nombre maximum de tokens pour la réponse. Defaults to 1500.\n",
    "\n",
    "    Returns:\n",
    "        str: Informations en string(réponse du LLM) structurées du CV au format JSON\n",
    "\n",
    "    Raises:\n",
    "        Exception: En cas d'erreur d'API ou de parsing JSON\n",
    "    \"\"\"\n",
    "    try:\n",
    "        reformulation_prompt = f\"\"\"\n",
    "        À partir de cette offre d'emploi, extrais les informations suivantes au format JSON. \n",
    "        Si une information n'est pas explicitement mentionnée dans le texte, retourne `null` ou une liste vide.\n",
    "        Ne devine pas les informations manquantes et ne retourne que ce qui est clairement présent dans le texte.\n",
    "\n",
    "        Format JSON attendu :\n",
    "        {{\n",
    "            \"Formation\": [\n",
    "                {{\n",
    "                    \"niveau_etudes\": \"str\",  // Ex: \"Licence\", \"Master\", \"Doctorat\"\n",
    "                    \"domaine_etudes\": [\"str\"]  // Ex: [\"Informatique\", \"Mathématiques\"]\n",
    "                }}\n",
    "            ],\n",
    "            \"Competences\": [\"str\"],  // Ex: [\"Python\", \"Gestion de projet\"]\n",
    "            \"Experiences\": [\n",
    "                {{\n",
    "                    \"domaine_activite\": [\"str\"],  // Ex: [\"Tech\", \"Finance\"]\n",
    "                    \"poste_occupe\": \"str\",  // Ex: \"Développeur Python\"\n",
    "                    \"duree\": \"str\"  // Ex: \"2 ans\"\n",
    "                }}\n",
    "            ],\n",
    "            \"Profil\": {{\n",
    "                \"titre\": \"str\",  // Ex: \"Développeur Full-Stack\"\n",
    "                \"disponibilite\": \"str\"  // Ex: \"dd-mm-yyyy\"\n",
    "            }}\n",
    "        }}\n",
    "\n",
    "        Texte de l'offre d'emploi :\n",
    "        \"{offre}\"\n",
    "\n",
    "        Ne retourne que le JSON, sans commentaires supplémentaires.\n",
    "        \"\"\"\n",
    "\n",
    "        offre_reformuler = litellm.completion(\n",
    "            model=\"mistral/mistral-medium\",\n",
    "            messages=[{\"role\": \"user\", \"content\": reformulation_prompt}],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            api_key=os.getenv(\"MISTRAL_API_KEY\"),\n",
    "        )\n",
    "\n",
    "        # Extraire et parser le JSON de la réponse\n",
    "        resultat = offre_reformuler[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        return resultat\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Erreur lors de l'analyse du CV: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Formation\": [\n",
      "    {\n",
      "      \"niveau_etudes\": \"Master\",\n",
      "      \"domaine_etudes\": [\"Informatique\", \"Mathématiques appliquées\"]\n",
      "    }\n",
      "  ],\n",
      "  \"Competences\": [\"Python\", \"SQL\", \"Terraform\", \"Bitbucket\", \"Jenkins\", \"Airflow\", \"Docker\", \"Kubernetes\", \"GCP (BigQuery, Spark DataProc)\", \"Apprentissage statistique\", \"Développement logiciel agile\"],\n",
      "  \"Experiences\": [\n",
      "    {\n",
      "      \"domaine_activite\": [\"Data Science\"],\n",
      "      \"poste_occupe\": \"Data Scientist\",\n",
      "      \"duree\": \"5 ans\"\n",
      "    }\n",
      "  ],\n",
      "  \"Profil\": {\n",
      "    \"titre\": \"Data Scientist\",\n",
      "    \"disponibilite\": null\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "job_offer_reforumer = analyze_cv_offre_emploi(job_offer)\n",
    "print(job_offer_reforumer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarité cosinus globale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Token d'API Hugging Face\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "API_URL = \"https://api-inference.huggingface.co/pipeline/feature-extraction/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "\n",
    "def get_embedding(text: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Envoie une requête à l'API Hugging Face pour obtenir l'embedding d'un texte.\n",
    "\n",
    "    Args:\n",
    "        text (str): Texte à encoder.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Embedding du texte.\n",
    "    \"\"\"\n",
    "    headers = {\"Authorization\": f\"Bearer {os.getenv(\"HF_TOKEN\")}\"}\n",
    "    response = requests.post(\n",
    "        \"https://api-inference.huggingface.co/pipeline/feature-extraction/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "        headers=headers,\n",
    "        json={\"inputs\": text, \"options\": {\"wait_for_model\": True}},\n",
    "    )\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Erreur API : {response.status_code}, {response.text}\")\n",
    "    return np.array(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarité globale entre le CV et l'offre d'emploi: 0.9035\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_similarity(text1: str, text2: str) -> float:\n",
    "    \"\"\"\n",
    "    Calcule la similarité cosinus entre deux textes en utilisant l'API Hugging Face.\n",
    "    \n",
    "    Args:\n",
    "        text1 (str): Premier texte.\n",
    "        text2 (str): Deuxième texte.\n",
    "    \n",
    "    Returns:\n",
    "        float: Score de similarité cosinus entre les deux textes.\n",
    "    \"\"\"\n",
    "    # Obtenir les embeddings des textes\n",
    "    embedding1 = get_embedding(text1)\n",
    "    embedding2 = get_embedding(text2)\n",
    "    \n",
    "    # Calculer la similarité cosinus\n",
    "    similarity = cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "    return similarity\n",
    "\n",
    "# Calculer la similarité entre le CV et l'offre d'emploi\n",
    "similarity_score = calculate_similarity(CV_reformuler, job_offer_reforumer)\n",
    "print(f\"Similarité globale entre le CV et l'offre d'emploi: {similarity_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversion en JSON-like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calcul de la similarité entre chaque partie du CV et l'offre d'emploi\n",
    "# Diviser le CV reformulé en json en plusieurs parties\n",
    "cv_json = json.loads(CV_reformuler)\n",
    "# print(cv_json)\n",
    "# convert job_offer to json\n",
    "job_offer_json = json.loads(job_offer_reforumer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Formation': [{'niveau_etudes': 'Master', 'domaine_etudes': ['Informatique', 'Mathématiques appliquées']}], 'Competences': ['Python', 'SQL', 'Terraform', 'Bitbucket', 'Jenkins', 'Airflow', 'Docker', 'Kubernetes', 'GCP (BigQuery, Spark DataProc)', 'Apprentissage statistique', 'Développement logiciel agile'], 'Experiences': [{'domaine_activite': ['Data Science'], 'poste_occupe': 'Data Scientist', 'duree': '5 ans'}], 'Profil': {'titre': 'Data Scientist', 'disponibilite': None}}\n"
     ]
    }
   ],
   "source": [
    "print(job_offer_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(cv_json)) # Les données sont en string JSON like\n",
    "print(type(job_offer_json)) # Les données sont en string JSON like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calcul des similarités cosinus par parties :\n",
    "\n",
    "- Formation\n",
    "- Competences\n",
    "- Experiences\n",
    "- Profil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarité entre les deux json like\n",
    "\n",
    "# Similarité Formation\n",
    "# print(cv_json.get(\"Formation\"))\n",
    "# print(job_offer_json.get(\"Formation\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarité entre Formation attendu et Formation du CV: 0.7965\n",
      "Similarité entre Compétences attendu et Compétences du CV: 0.8027\n",
      "Similarité entre Experiences attendu et Experiences du CV: 0.6901\n",
      "{'titre': 'Currently seeking an internship in Data Science', 'disponibilite': 'starting in March 2025 for a duration of 4 to 6 months.'}\n",
      "{'titre': 'Data Scientist', 'disponibilite': None}\n",
      "Similarité entre Profil attendu et Profil du CV: 0.5051\n"
     ]
    }
   ],
   "source": [
    "# Similarité entre Formation attendu et Formation du CV\n",
    "formation_candidat = cv_json.get(\"Formation\")\n",
    "formation_attendu = job_offer_json.get(\"Formation\")\n",
    "\n",
    "similarite_formation = calculate_similarity(str(formation_candidat), str(formation_attendu))\n",
    "print(f\"Similarité entre Formation attendu et Formation du CV: {similarite_formation:.4f}\")\n",
    "\n",
    "\n",
    "# Similarité entre Compétences attendu et Compétences du CV\n",
    "competences_candidat = cv_json.get(\"Competences\")\n",
    "# print(competences_candidat)\n",
    "competences_attendu = job_offer_json.get(\"Competences\")\n",
    "# print(competences_attendu)\n",
    "\n",
    "similarite_competences = calculate_similarity(str(competences_candidat), str(competences_attendu))\n",
    "print(f\"Similarité entre Compétences attendu et Compétences du CV: {similarite_competences:.4f}\")\n",
    "\n",
    "\n",
    "# Similarité entre Experiences attendu et Experiences du CV\n",
    "experiences_candidat = cv_json.get(\"Experiences\")\n",
    "# print(experiences_candidat)\n",
    "experiences_attendu = job_offer_json.get(\"Experiences\")\n",
    "# print(experiences_attendu)\n",
    "\n",
    "similarite_experiences = calculate_similarity(str(experiences_candidat), str(experiences_attendu))\n",
    "print(f\"Similarité entre Experiences attendu et Experiences du CV: {similarite_experiences:.4f}\")\n",
    "\n",
    "\n",
    "# Similarité entre Profil attendu et Profil du CV\n",
    "profil_candidat = cv_json.get(\"Profil\")\n",
    "print(profil_candidat)\n",
    "profil_attendu = job_offer_json.get(\"Profil\")\n",
    "print(profil_attendu)\n",
    "\n",
    "similarite_profil = calculate_similarity(str(profil_candidat), str(profil_attendu))\n",
    "print(f\"Similarité entre Profil attendu et Profil du CV: {similarite_profil:.4f}\")\n",
    "\n",
    "###################### Voir si on ne peut pas faire mieux (check if any au lieu d'une simalarité globale) ######################\n",
    "### Au lieu d'installer sentence transformers, utiliser une API huggingface avec pipeline et requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "challengeIA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
