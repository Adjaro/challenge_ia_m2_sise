{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  # For making HTTP requests (simulated scraping)\n",
    "from bs4 import BeautifulSoup  # For parsing HTML (simulated scraping)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "class ScrapingFactory:\n",
    "    domain = [\n",
    "        \"francetravail.fr\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def urlCheckInDomain(self, url):\n",
    "        \"\"\"\n",
    "        Check if the given URL belongs to one of the supported domains.\n",
    "        \n",
    "        :param url: The URL to check.\n",
    "        :return: True if the URL belongs to a supported domain, False otherwise.\n",
    "        \"\"\"\n",
    "        for domain in self.domain:\n",
    "            if domain in url:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def scrapOne(self, url):\n",
    "        \"\"\"\n",
    "        Scrape data from a single URL.\n",
    "        \n",
    "        :param url: The URL to scrape.\n",
    "        :return: Scraped data or None if the URL is not supported.\n",
    "        \"\"\"\n",
    "        if not self.urlCheckInDomain(url):\n",
    "            print(f\"URL {url} is not supported.\")\n",
    "            return None\n",
    "\n",
    "        if \"francetravail.fr\" in url:\n",
    "            scraper = ScrapingFrancetravail()\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        return scraper.scrape(url)\n",
    "\n",
    "    def scrapMany(self, keyWord):\n",
    "        \"\"\"\n",
    "        Scrape multiple job listings based on a keyword.\n",
    "        \n",
    "        :param keyWord: The keyword to search for.\n",
    "        :return: A list of scraped job listings.\n",
    "        \"\"\"\n",
    "        if not keyWord:\n",
    "            print(\"Keyword is required.\")\n",
    "            return []\n",
    "\n",
    "        # Use the France Travail scraper for this example\n",
    "        scraper = ScrapingFrancetravail()\n",
    "        return scraper.scrapMany(keyWord)\n",
    "\n",
    "\n",
    "\n",
    "class ScrapingFrancetravail:\n",
    "    # En-têtes pour imiter un vrai navigateur\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://www.francetravail.fr\"  # Base URL for France Travail\n",
    "\n",
    "    def scrape(self, url):\n",
    "        \"\"\"\n",
    "        Scrape data from a single France Travail URL.\n",
    "        \n",
    "        :param url: The URL to scrape.\n",
    "        :return: Scraped data as a dictionary or None if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Make a request to the URL\n",
    "            print(f\"Scraping France Travail URL: {url}\")\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()  # Raise an error for bad status codes\n",
    "\n",
    "            # Parse the HTML content\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Extract relevant data\n",
    "            job_title = soup.find(\"h1\").text.strip() if soup.find(\"h1\") else \"No Title\"\n",
    "            job_description = soup.find(\"div\", class_=\"description\").text.strip() if soup.find(\"div\", class_=\"description\") else \"No Description\"\n",
    "            company = soup.find(\"span\", class_=\"company-name\").text.strip() if soup.find(\"span\", class_=\"company-name\") else \"No Company\"\n",
    "            location = soup.find(\"span\", class_=\"job-location\").text.strip() if soup.find(\"span\", class_=\"job-location\") else \"No Location\"\n",
    "\n",
    "            return {\n",
    "                \"title\": job_title,\n",
    "                \"company\": company,\n",
    "                \"location\": location,\n",
    "                \"description\": job_description,\n",
    "                \"url\": url\n",
    "            }\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error scraping {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def scrapMany(self, keyWord, max_results=10):\n",
    "        \"\"\"\n",
    "        Scrape multiple job listings based on a keyword.\n",
    "        \n",
    "        :param keyWord: The keyword to search for.\n",
    "        :param max_results: The maximum number of results to return.\n",
    "        :return: A list of scraped job listings.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Construct the search URL\n",
    "            search_url = f\"{self.base_url}/recherche?query={keyWord}\"\n",
    "            print(f\"Searching France Travail for keyword: {keyWord}\")\n",
    "            response = requests.get(search_url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Parse the search results\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            job_listings = []\n",
    "\n",
    "            # Find job listing elements\n",
    "            job_elements = soup.find_all(\"div\", class_=\"job-listing\", limit=max_results)\n",
    "            for job in job_elements:\n",
    "                # Extract job link\n",
    "                job_link = job.find(\"a\")[\"href\"]\n",
    "                full_job_url = urljoin(self.base_url, job_link)\n",
    "\n",
    "                # Scrape individual job details\n",
    "                job_data = self.scrape(full_job_url)\n",
    "                if job_data:\n",
    "                    job_listings.append(job_data)\n",
    "\n",
    "            return job_listings\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error searching for jobs: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping France Travail URL: https://candidat.francetravail.fr/offres/recherche/detail/189KKQR\n",
      "Single URL Result: {'title': 'Offre n° 189KKQRDéveloppeur Full Stack .Net  (H/F)', 'company': 'No Company', 'location': 'No Location', 'description': \"Rejoignez une équipe sympathique et dynamique, dans un groupe en forte croissance !\\n\\nPOSTE et MISSION\\n\\nAu sein du pôle «Développement et R&D», vous serez intégré-e à l'équipe chargée du développement de nos Suites logicielles de gestion des risques Qualité, Sécurité, Environnement. A ce titre, vous travaillerez sur la conception et le développement de nos logiciels (algorithmes, IHM, API, persistance, Front End, Back End, etc) :\\n\\n- Intégrez une petite équipe d'ingénieurs, stable, qui travaille en étroite collaboration avec les autres services (notamment métier), selon la méthode Agile (Scrum), sur des projets durables,\\n\\n- A partir de spécifications fonctionnelles, vous concevez et développez les évolutions et nouvelles fonctionnalités de nos applications web en ASP.Net MVC (C#, Javascript, frontEnd) et de notre moteur métier (en C#, C++, backEnd),\\n\\n- Vous préparez avec l'équipe, la prochaine génération de notre application (technologies, langages, frameworks, processus seront à définir). Vos expériences sont des atouts !\\n\\n- Vous contribuez à la veille technologique, à la stratégie de tests, et à la hotline de 2ème niveau (en soutien à l'équipe Hotline)\\n\\n- Travail en équipe, autonomie, rigueur, souplesse et sens du service client sont indispensables\\n\\nNous utilisons actuellement Visual Studio 2022, GIT, Jira, Azure DevOps, N8N (Middleware). Nous nous appuyons sur une base de données objet propriétaire et SQL Server. Nous gérons les évolutions de nos solutions en client lourd, Web hébergé - Saas et responsive.\\n\\nLieu de travail et télétravail à convenir, en France métropolitaine\\n\\nRémunération à convenir (35 à 50 K€ selon parcours). Mutuelle avantageuse, chèques-déjeuner, avantages CSE. Idéalement temps plein. Horaires de bureau, pas d'astreinte. Opportunités de carrières, mobilité à l'International.\", 'url': 'https://candidat.francetravail.fr/offres/recherche/detail/189KKQR'}\n",
      "Searching France Travail for keyword: developer\n",
      "Error searching for jobs: 404 Client Error: Not Found for url: https://www.francetravail.fr/recherche?query=developer\n",
      "Multiple URL Results:\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of ScrapingFactory\n",
    "factory = ScrapingFactory()\n",
    "\n",
    "# Test scraping a single URL\n",
    "single_url = \"https://candidat.francetravail.fr/offres/recherche/detail/189KKQR\"\n",
    "single_result = factory.scrapOne(single_url)\n",
    "print(\"Single URL Result:\", single_result)\n",
    "\n",
    "# Test scraping multiple job listings based on a keyword\n",
    "keyword = \"developer\"\n",
    "multiple_results = factory.scrapMany(keyword)\n",
    "print(\"Multiple URL Results:\")\n",
    "for result in multiple_results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "challenge_ia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
